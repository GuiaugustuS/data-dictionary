{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, when, col\n",
    "from pyspark.sql.functions import count as _count\n",
    "from pyspark.sql.functions import min as _min\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import multiprocessing\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_memory = psutil.virtual_memory().total / (1024**3)\n",
    "memory_alocated = round(total_memory / 2)\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f'{total_memory} disponível. {memory_alocated} será alocada nesse processo. {num_cores} núcleos do processador serão envolvidos no processo')\n",
    "spark = \"\"\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"source_archive\") \\\n",
    "    .config(\"spark.executor.memory\", f\"{memory_alocated}g\") \\\n",
    "    .config(\"spark.executor.instances\", f\"{num_cores}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "root_path = '/content/drive/MyDrive/HDI_DR/Arquivos_Source_Individuais/input_caracteres'\n",
    "output_path = '/content/drive/MyDrive/HDI_DR/Arquivos_Source_Individuais/output_caracteres'\n",
    "\n",
    "csv_files = []\n",
    "\n",
    "parquet_files = []\n",
    "\n",
    "data = []\n",
    "\n",
    "total_columns = 0\n",
    "\n",
    "providers = ['Agger', 'Infocap', 'Quiver', 'Segfy']\n",
    "\n",
    "columns = ['Provider', 'ArchiveName', 'Headers', 'Cols','CountColumns']\n",
    "\n",
    "def valid_names(provider, sep):\n",
    "    for folder in os.listdir(root_path):\n",
    "    # Para cada diretório no caminho 'root_path'\n",
    "        if folder.startswith(provider):\n",
    "        # Se o início do nome da pasta corresponder com os nomes da lista 'providers'  \n",
    "            files_list = os.listdir(os.path.join(root_path, folder))\n",
    "            # file_list será a junção do caminho 'root_path' com a o nome da pasta 'folder'\n",
    "            for archive in files_list:\n",
    "            # Lista os arquivos da lista 'files_list'    \n",
    "                if archive.endswith('.csv'):\n",
    "                # Se o arquivo começa com '.csv'    \n",
    "                    csv_path = os.path.join(root_path, folder, archive)\n",
    "                    # 'csv_path' será a junção do caminho 'root_path', da pasta 'folder', e do arquivo 'file'.\n",
    "                    print(f'Caminho: {csv_path} \\n')\n",
    "                    df = spark.read.csv(csv_path, sep=sep, encoding='utf-8', header=True)\n",
    "                    # Lê o arquivo CSV usando Spark e especifica o separador e a codificação\n",
    "                    for column in df.columns:\n",
    "                    # Para cada coluna do DataFrame:\n",
    "                        df = df.withColumn(f'{column}_caract', length(df[column]))\n",
    "                        # Adiciona uma nova coluna com o sufixo '_caract' que contém o tamanho de cada valor da coluna original\n",
    "\n",
    "                elif archive.endswith('.parquet'):\n",
    "                # Se o arquivo começa com .parquet\n",
    "                    parquet_path = os.path.join(root_path, folder, archive)\n",
    "                    # 'parquet_path' será a junção do caminho 'root_path', da pasta 'folder', e do arquivo 'file'.\n",
    "                    print(f'Caminho: {parquet_path} \\n')\n",
    "                    \n",
    "                    df = spark.read.parquet(parquet_path)\n",
    "                    # Lê o arquivo PARQUET usando Spark e especifica o separador e a codificação\n",
    "                    for column in df.columns:\n",
    "                    # Para cada coluna do DataFrame: \n",
    "                        df = df.withColumn(f'{column}_caract', length(df[column]))\n",
    "                        # Adiciona uma nova coluna com o sufixo '_caract' que contém o tamanho de cada valor da coluna original\n",
    "\n",
    "                print(df)\n",
    "                colunas_selecionadas = [col for col in df.columns if col.endswith('_caract')]\n",
    "                # Seleciona apenas as colunas que terminam com '_caract'\n",
    "                df = df.select(*colunas_selecionadas)\n",
    "                # o DataFrame df é atualizado para conter apenas as colunas especificadas em colunas_selecionadas\n",
    "\n",
    "                temp_paths = []\n",
    "                # Cria uma lista de caminhos temporários\n",
    "\n",
    "                now = datetime.now().strftime(\"%d.%m.%Y_%H.%M.%S\")\n",
    "                # A variável 'now' obtém a data e horário do sistema\n",
    "                temp_save_path = os.path.join(output_path, f\"_temp_{now}\")\n",
    "                # Concatena o caminho do diretório de saída (output_path) com o nome do arquivo temporário. \n",
    "                temp_paths.append(temp_save_path)\n",
    "                # Adiciona o diretório temporário à lista 'temp_paths'\n",
    "\n",
    "                print('Salvando o novo prepared...')\n",
    "                # Salva o DataFrame resultante como um arquivo CSV em um diretório temporário\n",
    "                df.coalesce(1).write.format(\"csv\").option(\"header\", True).save(temp_save_path)\n",
    "                # O DataFrame df é modificado usando o método coalesce(1) que é utilizado para reduzir o número de partições do DataFrame e cria um arquivo CSV no caminho de salvamento temporário especificado.\n",
    "                temp_paths.append(temp_save_path)\n",
    "                # Adiciona o diretório temporário à lista 'temp_paths'\n",
    "                print(temp_paths)\n",
    "\n",
    "                temp_saved_paths = []\n",
    "\n",
    "                for path in temp_paths:\n",
    "                # Para cada pasta em 'temp_paths':\n",
    "                    now = datetime.now().strftime(\"%d.%m.%Y_%H.%M.%S\")\n",
    "                    # A variável 'now' obtém a data e horário do sistema\n",
    "                    files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "                    # Seleciona apenas os arquivos que terminam com '.csv'\n",
    "                    for i, file in enumerate(files):\n",
    "                    # Percorre cada arquivo na lista files e acessa tanto o índice quanto o valor do arquivo.\n",
    "                        temp_file_path = os.path.join(path, file)\n",
    "                        # Obtém todos os arquivos CSV no diretório temporário\n",
    "                        new_file_name = f\"{provider}_count_{archive}.csv\"\n",
    "                        # Cria o novo nome do arquivo concatenando as variáveis provider, archive.\n",
    "                        print(f'Este é o: {new_file_name}')\n",
    "                        temp_saved_path = os.path.join(output_path, new_file_name)\n",
    "                        # Concatena o caminho do diretório de saída (output_path) com o nome do arquivo de destino (new_file_name). O resultado é o caminho completo para o arquivo com o novo nome.\n",
    "                        os.rename(temp_file_path, temp_saved_path)\n",
    "                        # Renomeia o arquivo temporário com o novo nome e move para o diretório de saída\n",
    "                        temp_saved_paths.append(temp_saved_path)\n",
    "                        # Adiciona o caminho completo do arquivo salvo à lista temp_saved_paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_names('Agger',',')\n",
    "# Chama a função e especifica os parâmentros 'fornecedor', e 'separador'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas são as principais ações realizadas pelo código:\n",
    "\n",
    "As bibliotecas necessárias, como os, csv e pandas, são importadas.\n",
    "\n",
    "Variáveis como root_path, output_path, csv_files, parquet_files, data, total_columns, providers e columns são definidas.\n",
    "\n",
    "A função valid_names(provider, sep) é definida.\n",
    "\n",
    "O código itera sobre os diretórios no caminho root_path usando os.listdir(root_path).\n",
    "\n",
    "Verifica se o nome do diretório começa com um dos nomes na lista providers usando folder.startswith(provider).\n",
    "\n",
    "Dentro do diretório, itera sobre os arquivos no diretório usando os.listdir(os.path.join(root_path, folder)).\n",
    "\n",
    "Se o arquivo tiver a extensão .csv, lê o arquivo usando Spark e adiciona uma nova coluna para cada coluna existente com o sufixo _caract, que contém o tamanho de cada valor da coluna original.\n",
    "\n",
    "Se o arquivo tiver a extensão .parquet, lê o arquivo usando Spark e adiciona uma nova coluna para cada coluna existente com o sufixo _caract, que contém o tamanho de cada valor da coluna original.\n",
    "\n",
    "Seleciona apenas as colunas que terminam com _caract.\n",
    "\n",
    "Cria um diretório temporário usando a data e hora atual.\n",
    "\n",
    "Salva o DataFrame resultante como um arquivo CSV no diretório temporário.\n",
    "\n",
    "Renomeia os arquivos CSV no diretório temporário com um novo nome e move-os para o diretório de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_path = r'/content/drive/MyDrive/HDI_DR/Arquivos_Source_Individuais/output_caracteres'\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in os.listdir(root_path):\n",
    "# Itera sobre os arquivos no diretório root_path\n",
    "    provider = file.split('_')[0]\n",
    "    # Extrai o nome do fornecedor a partir do nome do arquivo, fatiando o nome em partes separadas pelo caractere \"_\", e selecionando a primeira ocorrência encontrada.\n",
    "    print(provider)\n",
    "    df = pd.read_csv(os.path.join(root_path, file))\n",
    "    # Combina o caminho da pasta principal (root_path) com o nome do arquivo (file), formando o caminho completo para o arquivo CSV e armazena no DataFrame 'df'.\n",
    "    df['provider'] = provider\n",
    "    # Adiciona uma coluna 'provider' com o nome do fornecedor.\n",
    "    df_list.append(df)\n",
    "\n",
    "for df in df_list:\n",
    "# Itera sobre os DataFrames na lista df_list\n",
    "    fornecedor = str(df['provider'].iloc[0])\n",
    "    # Obtém o valor da primeira linha dessa coluna.\n",
    "    print(f'Este é o fornecedor: {fornecedor} \\n')\n",
    "    conteudo_df = []\n",
    "    for col in df.columns:\n",
    "    # Itera sobre as colunas do DataFrame\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        # Verifica se a coluna contém valores numéricos\n",
    "            valores_numericos = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Converte os valores para tipo numérico.\n",
    "            # O parâmetro errors='coerce' é utilizado para lidar com valores não numéricos. Caso ocorram valores não numéricos na coluna, eles serão convertidos para NaN (valor ausente). \n",
    "            linha = (fornecedor, col, round(df[col].mean(),0), round(df[col].max(),0))\n",
    "            # Cria uma tupla contendo informações estatísticas para cada coluna do DataFrame df.\n",
    "            # Em seguida, aplica round() para arredondar o resultado para zero casas decimais.\n",
    "            conteudo_df.append(linha)\n",
    "            # Adiciona a tupla linha à lista conteudo_df. \n",
    "\n",
    "df_final = pd.DataFrame(conteudo_df, columns=['provider','variavel','media_caract','max_caract'])\n",
    "# Cria um novo DataFrame com as informações coletadas\n",
    "\n",
    "df_final.to_csv(rf'/content/drive/MyDrive/HDI_DR/Arquivos_Source_Individuais/dicionario_colunas/{file}_numeros.csv', index=False) \n",
    "# Salva o DataFrame como um arquivo CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um resumo das principais ações realizadas pelo código:\n",
    "\n",
    "O código itera sobre os arquivos no diretório especificado em root_path.\n",
    "\n",
    "O nome do fornecedor é extraído do nome do arquivo usando o método split e armazenado na variável provider.\n",
    "\n",
    "O arquivo CSV é lido usando o Pandas e armazenado em um DataFrame chamado df.\n",
    "\n",
    "Uma coluna chamada 'provider' é adicionada ao DataFrame df com o nome do fornecedor.\n",
    "\n",
    "O DataFrame df é adicionado à lista df_list.\n",
    "\n",
    "O código itera sobre os DataFrames na lista df_list.\n",
    "\n",
    "O nome do fornecedor é extraído do DataFrame.\n",
    "\n",
    "O código cria uma lista vazia chamada conteudo_df para armazenar as informações sobre as colunas.\n",
    "\n",
    "O código itera sobre as colunas do DataFrame.\n",
    "\n",
    "Verifica se a coluna contém valores numéricos usando a função is_numeric_dtype do Pandas.\n",
    "\n",
    "Se a coluna contiver valores numéricos, os valores são convertidos para tipo numérico usando a função to_numeric do Pandas.\n",
    "\n",
    "O código cria uma linha com informações sobre a coluna, como o fornecedor, nome da variável, média dos valores e máximo dos valores.\n",
    "\n",
    "A linha é adicionada à lista conteudo_df.\n",
    "\n",
    "O código cria um novo DataFrame chamado df_final com base nas informações coletadas.\n",
    "\n",
    "O DataFrame df_final é salvo como um arquivo CSV em um diretório específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empilhar os arquivos\n",
    "\n",
    "\n",
    "root_path = r'/content/drive/MyDrive/HDI_DR/Arquivos_Source_Individuais/dicionario_colunas/'\n",
    "# Caminho para os arquivos\n",
    "\n",
    "df_list = []\n",
    "# Lista vazia para armazenar os DataFrames\n",
    "\n",
    "columns = ['provider','variavel','media_caract','max_caract']\n",
    "# Nomes das colunas do DataFrame final\n",
    "\n",
    "for file in os.listdir(root_path):\n",
    "# Itera sobre os arquivo do diretório 'root_path'\n",
    "  file_path = os.path.join(root_path, file)\n",
    "  # file_path será a junção do caminho 'root_path' com o nome do arquivo 'file'\n",
    "  if file_path.endswith('.csv'):\n",
    "  # Se o arquivo começa com '.csv'    \n",
    "    df = pd.read_csv(file_path)\n",
    "    # Faz a leitura dos arquivos e armazena na variável 'df'\n",
    "    rows = df.iloc[1:]\n",
    "    # Pega apenas da segunda linha em dia, não pega o cabeçalho das colunas.\n",
    "    print(rows)\n",
    "    df_list.append(df)\n",
    "    # Adiciona as linhas à lista 'df_list'\n",
    "\n",
    "combined_df = pd.concat(df_list)\n",
    "# Concatena os DataFrames individuais da lista 'df_list' e atribui o resultado à variável 'combined_df'.\n",
    "combined_df.columns = columns\n",
    "# Especifica que as colunas do DataFrame terão os nomes que constam na lista 'columns'\n",
    "\n",
    "print(combined_df)\n",
    "\n",
    "\n",
    "combined_df.to_csv(r'/content/drive/MyDrive/HDI_DR/Arquivos_Source_Individuais/dicionario_colunas/amostra_final.csv',index=False, sep=',', encoding='utf-8',header=True)\n",
    "# Exporta o arquivo para csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
